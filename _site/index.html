<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="google-site-verification" content="C8cF_zqG3_SNAMMEu93SHHse0M-qbabeENCFMfVxvoo" />
	<meta charset="utf-8">
	<title>Samuel (Sam) Daulton</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Samuel (Sam) Daulton" property="og:site_name">

    <meta content="Samuel (Sam) Daulton" property="og:title">


    <meta content="website" property="og:type">


    <meta content="Personal website
" property="og:description">


    <meta content="http://localhost:4000/" property="og:url">



    <meta content="http://localhost:4000/assets/img/samuel-daulton.jpg" property="og:image">




    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@samjdaulton">
    <meta name="twitter:creator" content="@samjdaulton">

    <meta name="twitter:title" content="Samuel (Sam) Daulton">


    <meta name="twitter:url" content="http://localhost:4000/">


    <meta name="twitter:description" content="Personal website
">


    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/samuel-daulton.jpg">


	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700|Lato:300,400,700&display=swap" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
  <!-- Styles -->
  <link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/samuel-daulton.jpg" alt="Samuel (Sam) Daulton"></a>
      </div>
      <div class="author-name">Samuel (Sam) Daulton</div>
      <p></p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>

          <li><a href="https://scholar.google.com/citations?user=beXm1FwAAAAJ" target="_blank"><i class="fa fa-google" aria-hidden="true"></i></a></li>




          <li class="github"><a href="http://github.com/sdaulton" target="_blank"><i class="fa fa-github"></i></a></li>


          <li class="linkedin"><a href="https://in.linkedin.com/in/samuel-daulton" target="_blank"><i class="fa fa-linkedin"></i></a></li>



          <li><a href="https://twitter.com/samjdaulton" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>

      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2022 &copy; Samuel (Sam) Daulton</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <style>
  img {
    width: 25%;
    float: left;
    margin-right:1rem;
  }
  p {
    overflow: hidden;
  }
</style>
<div class="container">

  <div class="col-xs-12 col-sm-9 col-md-9">

    <div class="row">
    <p>
    I am a <a href='https://research.fb.com/people/daulton-sam/'>research scientist at Meta</a> on the <a href='https://research.fb.com/teams/core-data-science/'>Core Data Science</a> team, PhD candidate in machine learning at the Universtiy of Oxford, and co-creator of <a href='https://botorch.org'>BoTorch</a>---an open source library for Bayesian optimization research. Within Core Data Science, I work in the <a href='https://ai.facebook.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/'>Adaptive Experimentation</a> research group. I am a member of the <a href='https://www.robots.ox.ac.uk/~parg/'>Machine Learning Research Group</a> at Oxford. During my PhD, I am working with <a href='https://www.robots.ox.ac.uk/~mosb/'>Michael Osborne</a> (Oxford), <a href='http://eytan.github.io'>Eytan Bakshy</a> (Meta), and Max Balandat (Meta). My research focuses on methods for principled, sample-efficient optimization including Bayesian optimization and transfer learning. I am particularly interested in practical methods for principled exploration (using probablistic models) that are are robust across applied problems and depend on few, if any, hyperparameters. Furthermore, I aim to democratize such methods by open sourcing reproducible code. Prior to joining Meta, I worked with <a href='https://finale.seas.harvard.edu/'>Finale Doshi-Velez</a> at Harvard University on efficient and robust methods for transfer learning.
    </p>
    In my free time, it's a safe bet that I'm climbing, skiing, running, scuba diving, or scheming about how to get to the remote reaches of the world.
    </div>

    <div class="row">
      <h2><a name="publications"></a>Selected Papers</h2>
      <hr>
      <h3>Preprints</h3>
      <a href="/"><img src="/assets/img/robust_mobo_img.png" alt="Samuel (Sam) Daulton"></a>
      <p>
        <strong>Robust Multi-Objective Bayesian Optimization Under Input Noise</strong>
        <br>
        Bayesian optimization (BO) is a sample-efficient approach for tuning design parameters to optimize expensive-to-evaluate, black-box performance metrics. In many manufacturing processes, the design parameters are subject to random input noise, resulting in a product that is often less performant than expected. Although BO methods have been proposed for optimizing a single objective under input noise, no existing method addresses the practical scenario where there are multiple objectives that are sensitive to input perturbations. In this work, we propose the first multi-objective BO method that is robust to input noise. We formalize our goal as optimizing the multivariate value-at-risk (MVaR), a risk measure of the uncertain objectives. Since directly optimizing MVaR is computationally infeasible in many settings, we propose a scalable, theoretically-grounded approach for optimizing MVaR using random scalarizations. Empirically, we find that our approach significantly outperforms alternative methods and efficiently identifies optimal robust designs that will satisfy specifications across multiple metrics with high probability.
        <br>
        <strong>Samuel Daulton</strong>*, <a href='https://www.saitcakmak.com/'>Sait Cakmak</a>* (*<em>equal contribution</em>), <a href='https://research.fb.com/people/balandat-max/'>Maximilian Balandat</a>, <a href='https://www.robots.ox.ac.uk/~mosb/'>Michael A. Osborne</a>, <a href='https://www.enluzhou.gatech.edu/'>Enlu Zhou</a>, <a href='http://eytan.github.io'>Eytan Bakshy</a>
        <br>
        <em>ArXiv</em>, 2022.
        <br>
        <a href="https://arxiv.org/abs/2202.07549" class='btn btn-default'>Paper</a> | <a href="https://github.com/facebookresearch/robust_mobo" class='btn btn-default'>Code</a>
      </p>
      <a href="/"><img src="/assets/img/MORBO.png" alt="Samuel (Sam) Daulton"></a>
      <p>
        <strong>Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces</strong>
        <br>
        The ability to optimize multiple competing objective functions with high sample efficiency is imperative in many applied problems across science and industry. Multi-objective Bayesian optimization (BO) achieves strong empirical performance on such problems, but even with recent methodological advances, it has been restricted to simple, low-dimensional domains. Most existing BO methods exhibit poor performance on search spaces with more than a few dozen parameters. In this work we propose MORBO, a method for multi-objective Bayesian optimization over high-dimensional search spaces. MORBO performs local Bayesian optimization within multiple trust regions simultaneously, allowing it to explore and identify diverse solutions even when the objective functions are difficult to model globally. We show that MORBO significantly advances the state-of-the-art in sample-efficiency for several high-dimensional synthetic and real-world multi-objective problems, including a vehicle design problem with 222 parameters, demonstrating that MORBO is a practical approach for challenging and important problems that were previously out of reach for BO methods.
        <br>
        <strong>Samuel Daulton</strong>*, <a href='https://dme65.github.io/'>David Eriksson</a>* (*<em>equal contribution</em>), <a href='https://research.fb.com/people/balandat-max/'>Maximilian Balandat</a>, <a href='http://eytan.github.io'>Eytan Bakshy</a>
        <br>
        <em>ArXiv</em>, 2021.
        <br>
        <a href="https://arxiv.org/abs/2109.10964" class='btn btn-default'>Paper</a>
      </p>
      <hr>
      <h3>Conference Publications</h3>
      <a href="/"><img src="/assets/img/nehvi_thumbnail.png" alt="Samuel (Sam) Daulton"></a>
      <p>
        <strong>Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement</strong>
        <br>
        Failure to account for noisy observations at the previously evaluated designs can degrade optimization performance and lead to clumped solutions on the Pareto frontier.To address this issue, we propose a new acquisition function q-Noisy Expected Hypervolume improvement (qNEHVI) that takes a Bayesian approach and integrates over the unknown function values at the previously evaluated points. qNEHVI is one-step Bayes optimal in noisy and noiseless environments (in the latter it is equivalent to expected hypervolume improvement). Although the iterated expectation in qNEHVI is intractable and computing qNEHVI involves computationally intensive, non-differentiable box decompositions, we show that qNEHVI can be efficiently estimated by caching the box decompositions (CBD) once per Bayesian optimization iteration and using sample-average approximation. Furthermore, we show how CBD can be used for efficient generating batches of candidate designs and reduces time/space complexity from exponential to polynomial in the batch size. We demonstrate state-of-the-art performance in sequential optimization, and we find that qNEHVI performs best regardless of the batch size.
        <br>
        <strong>Samuel Daulton</strong>, <a href='https://research.fb.com/people/balandat-max/'>Maximilian Balandat</a>, <a href='http://eytan.github.io'>Eytan Bakshy</a>
        <br>
        <em>Advances in Neural Information Processing Systems</em> 34, 2021.
        <br>
        <a href="https://arxiv.org/abs/2105.08195" class='btn btn-default'>Paper</a> | <a href="https://botorch.org/tutorials/multi_objective_bo" class='btn btn-default'>Code</a> | <a href="https://youtu.be/XOYo_TNo3Gw" class='btn btn-default'>Video</a>
      </p>
      <a href="/"><img src="/assets/img/qehvi.png" alt="Samuel (Sam) Daulton"></a>
      <p>
        <strong>Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization</strong>
        <br>
        We propose a new state-of-the-art acquisition function for constrained Bayesian optimization of multiple competing objectives. The acquisition function is an exact calculation of expected hypervolume improvement other than monte-carlo integration error. It enables parallel (batch) evaluations via the inclusion-exclusion principle, and it is differentiable, which enables efficient gradient-based acquisition optimization. We prove that the sample average gradient of the Monte-Carlo acquisition is an unbiased estimator of the gradient of the true expected hypervolume improvement.
        <br>
        <strong>Samuel Daulton</strong>, <a href='https://research.fb.com/people/balandat-max/'>Maximilian Balandat</a>, <a href='http://eytan.github.io'>Eytan Bakshy</a>
        <br>
        <em>Advances in Neural Information Processing Systems</em> 33, 2020.
        <br>
        <a href="https://arxiv.org/abs/2006.05078" class='btn btn-default'>Paper</a> | <a href="https://botorch.org/tutorials/multi_objective_bo" class='btn btn-default'>Code</a> | <a href="https://youtu.be/JNt_hBjH4wU" class='btn btn-default'>Video</a>
      </p>
      <a href="/"><img src="/assets/img/botorch_logo_lockup-01.jpg" alt="Samuel (Sam) Daulton"></a>
      <p>

        <strong>BoTorch: Programmable Bayesian Optimization in PyTorch</strong>
        <br>
        We propose a modular Monte-Carlo-based framework for developing new methods for Bayesian optimization. We include multiple examples including a novel one-shot optimization formulation of the Knowledge Gradient. We provide convergence guarantees for a broad class of quasi-Monte-Carlo acquisition functions using the sample average approximation.
        <br>
        <a href='https://research.fb.com/people/balandat-max/'>Maximilian Balandat</a>,
        <a href='https://research.fb.com/people/karrer-brian/'>Brian Karrer</a>,
        <a href='http://pitt.edu/~drjiang/'>Daniel Jiang</a>,
        <strong>Samuel Daulton</strong>,
        <a href='http://lethalletham.com/'>Benjamin Letham</a>, <a href='https://cims.nyu.edu/~andrewgw/'>Andrew Gordon Wilson</a>, <a href='http://eytan.github.io'>Eytan Bakshy</a>
        <br>
        <em>Advances in Neural Information Processing Systems</em> 33, 2020.
        <br>
        <a href="https://arxiv.org/abs/1910.06403" class='btn btn-default'>Paper</a> | <a href="https://botorch.org/tutorials/closed_loop_botorch_only" class='btn btn-default'>Code</a>
      </p>
      <br>
      <a href="/"><img src="/assets/img/hipmdp.jpg" alt="Samuel (Sam) Daulton"></a>
      <p>
        <strong>Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes</strong>
        <br>
        We propose a novel algorithm for transfering knowledge between similar, but different reinforcement learning tasks by learning low-dimensional latent embeddings. These embeddings encode task-specific nuances and are provided with the state and action as the input to transition model that is shared across task instances. Modeling the dynamics with a Bayesian neural network enables scalable inference and joint optimization of the network parameters and latent embeddings.
        <br>
        <a href='https://twkillian.github.io/'>Taylor W. Killian</a>*,
        <strong>Samuel Daulton* (*<em>equal contribution</em>)</strong>,
        <a href='https://cs.brown.edu/people/gdk/'>George Konidaris</a>, <a href='https://finale.seas.harvard.edu/'>Finale Doshi-Velez</a>
          <br>
          <strong>[Oral]</strong> <em>Advances in Neural Information Processing Systems</em> 30, 2017.
          <br>
          <a href="http://papers.neurips.cc/paper/7205-robust-and-efficient-transfer-learning-with-hidden-parameter-markov-decision-processes" class='btn btn-default'>Paper</a> | <a href="https://github.com/dtak/hip-mdp-public" class='btn btn-default'>Code</a>
      </p>
      <hr>
      <h3>Workshop Papers</h3>
      <a href="/"><img src="/assets/img/cv_nas.png" alt="Samuel (Sam) Daulton"></a>
      <p>
        <strong>Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization</strong>
        <br>
        When tuning the architecture and hyperparameters of large machine learning models for on-device deployment, it is desirable to understand the optimal trade-offs between on-device latency and model accuracy. In this work, we leverage recent methodological advances in Bayesian optimization over high-dimensional search spaces and multi-objective Bayesian optimization to efficiently explore these trade-offs for a production-scale on-device natural language understanding model at Facebook.
        <br>
        <a href='https://dme65.github.io/'>David Eriksson</a>*, Pierce I-Jen Chuang*, <strong>Samuel Daulton* (*<em>equal contribution</em>)</strong>, Peng Xia, Akshat Shrivastava, Arun Babu, Shicong Zhao, Ahmed Aly, Ganesh Venkatesh, <a href='https://research.fb.com/people/balandat-max/'>Maximilian Balandat</a>.
        <br>
        <em>ICML Workshop on Automated Machine Learning</em>, 2021.
        <br>
        <a href="https://arxiv.org/abs/2106.11890" class='btn btn-default'>Paper</a>
      </p>
      <br>
      <a href="/"><img src="/assets/img/dts.jpeg" alt="Samuel (Sam) Daulton"></a>
      <p>
        <strong>Distilled Thompson Sampling: Practical and Efficient Thompson Sampling via Imitation Learning</strong>
        <br>
        We propose a practical method for distilling a Thompson sampling policy into a compact, explicit policy representation for contextual bandit optimization  in applications with limited memory and low-latency requirements. The expensive posterior sampling and numerical optimization is performed offline, while the imitation policy is used for efficient online decision-making. We show that our method enjoys the same Bayes regret as the best UCB algorithm, up to a sum of single time step approximation errors, which can be easily controlled with abudant "unlabeled" contexts (without an action or reward) that are available in many practical applications (e.g. firms typically have databases with features about different entities).
        <br>
        <a href='https://hsnamkoong.github.io/'>Hongseok Namkoong</a>*, <strong>Samuel Daulton* (*<em>equal contribution</em>)</strong>, <a href='http://eytan.github.io'>Eytan Bakshy</a>
        <br>
        <strong>[Oral]</strong> <em>NeurIPS Offline Reinforcement Learning Workshop</em>, 2020.
        <br>
        <a href="https://arxiv.org/abs/2011.14266" class='btn btn-default'>Paper</a> | <a href="https://youtu.be/InpIX3sNeLQ" class='btn btn-default'>Video</a> | <a href="https://youtu.be/2-fGgoSs6Jk" class='btn btn-default'>Talk</a>
      </p>
      <br>
      <a href="/"><img src="/assets/img/cts.jpg" alt="Samuel (Sam) Daulton"></a>
      <p>
        <strong>Thompson Sampling for Contextual Bandit Problems with Auxiliary Safety Constraints</strong>
        <br>
        We consider a new class of contextual bandit problems with constraints on auxiliary outcome(s). We consider upper confidence bound and Thompson sampling-based algorithms and perform ablation studies revealing nice properties regarding fairness using the Thompson sampling algorithm. We demonstrate the performance of the algorithm on a real world video transcoding problem.
        <br>
        <strong>Samuel Daulton</strong>, <a href='https://research.fb.com/people/singh-shaun/'>Shaun Singh</a>, <a href='http://www.columbia.edu/~va2297/'>Vashist Avadhanula</a>, <a href='https://ddimmery.com/'>Drew Dimmery</a>, <a href='http://eytan.github.io'>Eytan Bakshy</a>
        <br>
        <em>NeurIPS Workshop on Safety and Robustness in Decision Making</em>, 2019.
        <br>
        <a href="https://arxiv.org/abs/1911.00638" class='btn btn-default'>Paper</a>
      </p>
      <br>
  </div>
  <br>
  <hr>

<div class="row">
  <h2><a name="Talks"></a>Talks</h2>
  <hr>
  <a href="/"><img src="/assets/img/netflix.png" alt="Samuel (Sam) Daulton"></a>
  <p>
    <strong>Practical Solutions to Exploration Problems</strong>
    <br>
    I discuss many practical approaches we use at Facebook for principled exploration including policy optimization with Bayesian optimization via online experiments, constrained Bayesian optimization, multi-task Bayesian optimization accelerated with offline simulations, and contextual bandits.
    <br>
    <strong>Samuel Daulton</strong>
    <br>
    <em>Netflix ML Platform Meetup on Exploration and Exploitation</em>, 2019.
    <br>
    <a href='https://youtu.be/A-JJvYaBPUU' class='btn btn-default'>Video</a> | <a href="https://www.slideshare.net/FaisalZakariaSiddiqi/facebook-talk-at-netflix-ml-platform-meetup-sep-2019" class='btn btn-default'>Slides</a>
  </p>
</div>


<footer>
&nbsp;
</footer>

<!--
<article class="post">

    <a class="post-thumbnail" style="background-image: url(/assets/img/i-rest.jpg)" href="/how-i-rest-from-work/"></a>

  <div class="post-content">
    <h2 class="post-title"><a href="/how-i-rest-from-work/">Botorch</a></h2>
    <p>Fam locavore snackwave bushwick +1 sartorial. Selfies portland knausgaard synth. Pop-up art party marfa deep...</p>
    <span class="post-date">2017, Sep 12&nbsp;&nbsp;&nbsp;—&nbsp;</span>
    <span class="post-words">4 minute read</span>
  </div>
</article>

<article class="post">

    <a class="post-thumbnail" style="background-image: url(/assets/img/software.jpg)" href="/the-best-organizer-software/"></a>

  <div class="post-content">
    <h2 class="post-title"><a href="/the-best-organizer-software/">The Best Organizer Software</a></h2>
    <p>Church-key blog messenger bag, selfies umami man braid mlkshk. Pork belly cornhole meditation tumblr meh...</p>
    <span class="post-date">2017, Sep 11&nbsp;&nbsp;&nbsp;—&nbsp;</span>
    <span class="post-words">3 minute read</span>
  </div>
</article>

<article class="post">

    <a class="post-thumbnail" style="background-image: url(/assets/img/how-to-start.jpg)" href="/how-to-start-programming/"></a>

  <div class="post-content">
    <h2 class="post-title"><a href="/how-to-start-programming/">How To Start Programming</a></h2>
    <p>Post-ironic jean shorts bushwick umami, synth beard austin hell of meh kitsch distillery sustainable plaid...</p>
    <span class="post-date">2017, Sep 11&nbsp;&nbsp;&nbsp;—&nbsp;</span>
    <span class="post-words">3 minute read</span>
  </div>
</article>

<article class="post">

    <a class="post-thumbnail" style="background-image: url(/assets/img/workflow.jpg)" href="/10-tips-to-improve-your-workflow/"></a>

  <div class="post-content">
    <h2 class="post-title"><a href="/10-tips-to-improve-your-workflow/">10 Tips To Improve Your Workflow</a></h2>
    <p>Asymmetrical portland enamel pin af heirloom ramps authentic thundercats. Synth truffaut schlitz aesthetic, palo santo...</p>
    <span class="post-date">2017, Sep 11&nbsp;&nbsp;&nbsp;—&nbsp;</span>
    <span class="post-words">5 minute read</span>
  </div>
</article>

<article class="post">

    <a class="post-thumbnail" style="background-image: url(/assets/img/js-1.png)" href="/conference-on-javascript/"></a>

  <div class="post-content">
    <h2 class="post-title"><a href="/conference-on-javascript/">Conference on Javascript</a></h2>
    <p>Jean shorts organic cornhole, gochujang post-ironic chicharrones authentic flexitarian viral PBR&amp;B forage wolf. Man braid...</p>
    <span class="post-date">2017, Sep 09&nbsp;&nbsp;&nbsp;—&nbsp;</span>
    <span class="post-words">2 minute read</span>
  </div>
</article>

<article class="post">


  <div class="post-content">
    <h2 class="post-title"><a href="/welcome-to-jekyll/">Welcome to Jekyll!</a></h2>
    <p>You’ll find this post in your _posts directory. Go ahead and edit it and re-build...</p>
    <span class="post-date">2017, Apr 06&nbsp;&nbsp;&nbsp;—&nbsp;</span>
    <span class="post-words">1 minute read</span>
  </div>
</article>


<div class="container">
  <nav class="pagination" role="pagination">
    <ul>





    </ul>
  </nav>
</div>
 -->

</div>

  </div>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
